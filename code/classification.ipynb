{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric and plotting libraries\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.nedeljkovic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# deep learning/vision libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if it's available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 region, all info about speakers, 4 sessions, white noise, 50 info per neuron (per speaker), 100 neurons per region => image 100x250(5x50) - 8 of them (2 states x 4 sessions) x 30 (sampling points over time)\n",
    "- final 240 images (120 per state)\n",
    "- oversampling input images with NaNs (maybe random oversampling)\n",
    "- combination of 2 different regions in one image/mice???\n",
    "- including bandpass noise into data representation \n",
    "- representing input images as different stimulus (100 x 50)\n",
    "---\n",
    "- training on combined sessions, different sessions (evaluating results on different sessions)\n",
    "- maybe training on the particular stimulus and then seeing wich stimulus is the most inforative for the state classification\n",
    "- performing classification on different regions to see which region (to see if some regions have very noninformative responses)\n",
    "- performin classification on different combination of regions???\n",
    "- comparing the classificator performance - input data is white noise/bandpass noise/combination of noises\n",
    "---\n",
    "- stimulus classificators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification between anest and awake state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucitavanje_podataka(directory):\n",
    "    id = 0\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        label = f.split(\"\\\\\")[-1][0:2]\n",
    "        id = id + 1\n",
    "        dat = np.load(f)\n",
    "        data.append([id, dat, label])\n",
    "    \n",
    "    #random.seed(2)\n",
    "    #random.shuffle(data)\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vektorizacija(podaci):\n",
    "    prevodjenje_labela = {\"an\": [1], \"aw\": [0]}  # TRUE:an  FALSE:aw\n",
    "\n",
    "    vektorizovani_podaci = []\n",
    "    for a in podaci:\n",
    "        vektorizovani_podaci.append([a[0], a[1], prevodjenje_labela[a[2]]])\n",
    "\n",
    "    return vektorizovani_podaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def podela_podataka(data_vectors):\n",
    "    X_train, y_train, X_test, y_test = [],[],[],[]\n",
    "    i,j = 0,0\n",
    "\n",
    "    random.shuffle(data_vectors)\n",
    "    for item in data_vectors:\n",
    "        if item[2]==[1]:\n",
    "            i = i + 1\n",
    "            if i <= 50:\n",
    "                X_test.append(item[1])\n",
    "                y_test.append(item[2])\n",
    "            else:\n",
    "                X_train.append(item[1])\n",
    "                y_train.append(item[2])\n",
    "        else:\n",
    "            j = j + 1\n",
    "            if  j<= 50:\n",
    "                X_test.append(item[1])\n",
    "                y_test.append(item[2])\n",
    "            else:\n",
    "                X_train.append(item[1])\n",
    "                y_train.append(item[2])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "putanja = r\"C:\\Users\\Zephyrus\\Desktop\\cnn data\\data\"\n",
    "#niz od podataka za svaku sekvencu (podatak je niz od id-ja, sekvence i labele)\n",
    "podaci = ucitavanje_podataka(putanja)\n",
    "\n",
    "vektorizovani_podaci = vektorizacija(podaci)\n",
    "X_train, y_train, X_test, y_test = podela_podataka(vektorizovani_podaci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape(300, 1, 110, 155)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test).reshape(100, 1, 110, 155)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test[:20]\n",
    "y_test2 = y_test[:20]\n",
    "\n",
    "X_test1 = X_test[20:]\n",
    "y_test1 = y_test[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "savеpath = \"C:/Users/Zephyrus/Desktop/cnn data/\"\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    np.save(savеpath + 'train/' + str(y_train[i][0]) + '/' + str(i) + '.npy', X_train[i])\n",
    "\n",
    "for i in range(len(X_test1)):\n",
    "    with open(savеpath + 'val/' + str(y_test1[i][0]) + '/' + str(i) + '.npy', 'wb') as f:\n",
    "        np.save(f, X_test1[i])\n",
    "\n",
    "for i in range(len(X_test2)):\n",
    "    with open(savеpath + 'test/' + str(y_test2[i][0]) + '/' + str(i) + '.npy', 'wb') as f:\n",
    "        np.save(f, X_test2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 16  # batch size of input\n",
    "\n",
    "# CONV_SIZE = 3    #first filter size\n",
    "# CONV_DEEP = 128   #number of first filter(convolution deepth)\n",
    "\n",
    "DROPOUT_KEEP_PROB = 0.5  # keep probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_numpy_list, data_transforms):\n",
    "        self.data_numpy_list = data_numpy_list\n",
    "        self.transform = data_transforms\n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        for ind in range(len(self.data_numpy_list)):\n",
    "            data_slice_file_name = self.data_numpy_list[ind]\n",
    "            data_i = np.load(data_slice_file_name)\n",
    "            idx = data_slice_file_name.rfind(\"\\\\\")\n",
    "            self.data_list.append(data_i)\n",
    "            self.label_list.append(int(data_slice_file_name[idx-1]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = np.asarray(self.data_list[index])\n",
    "        label = np.asarray(self.label_list[index])\n",
    "        data = torch.from_numpy(data)\n",
    "        label = torch.from_numpy(label)\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_numpy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder\n",
    "class CustomCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, (3, 3), stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 64, (3, 3), stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3, 3), stride=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, (3, 3), stride=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), stride=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, (3, 3), stride=1)\n",
    "        self.conv7 = nn.Conv2d(256, 512, (3, 3), stride=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, (3, 3), stride=1)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.norm4 = nn.BatchNorm2d(256)\n",
    "        self.norm5 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d((2, 2))\n",
    "      \n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "       \n",
    "\n",
    "        self.sf = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        y1 = self.conv1(x)\n",
    "        y1 = self.norm1(y1)\n",
    "        y2 = self.conv2(y1)\n",
    "        y3 = self.maxpool(y2)\n",
    "        y4 = self.conv3(y3)\n",
    "        y4 = self.norm3(y4)\n",
    "        y5 = self.conv4(y4)\n",
    "        y6 = self.maxpool(y5)\n",
    "        y7 = self.conv5(y6)\n",
    "        y7 = self.norm4(y7)\n",
    "        y8 = self.conv6(y7)\n",
    "        y9 = self.maxpool(y8)\n",
    "        y10 = self.conv7(y9)\n",
    "        y11 = self.conv8(y10)\n",
    "        y12 = self.conv8(y11)\n",
    "        y13 = self.maxpool(y12)\n",
    "        y14 = torch.flatten(y13, 1)\n",
    "        y15 = self.fc1(y14)\n",
    "        y16 = self.fc2(y15)\n",
    "        y17 = self.fc3(y16)\n",
    "        y = self.sf(y17)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs, path, dataloaders):\n",
    "    start_time = time.time()\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            best_acc = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels.long())\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                    if(preds[i] == labels.data[i]):\n",
    "                        if(preds[i] == 1):\n",
    "                            tp += 1\n",
    "                        else:\n",
    "                            tn += 1\n",
    "                    else:\n",
    "                        if (preds[i] == 1):\n",
    "                            fp += 1\n",
    "                        else:\n",
    "                            fn += 1\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels.data).sum()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_acc = 100 * float(correct) / total\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "\n",
    "            metrics[phase + \"_loss\"].append(epoch_loss)\n",
    "            metrics[phase + \"_acc\"].append(epoch_acc)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {(time_elapsed // 60):.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc: .4f}')\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "    print(f'Sens: {sensitivity: .4f} Spec: {specificity: .4f} F1: {f1: .4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "    # load best model weights\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, criterion ,optimizer, PATH, podaci):\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    phase = 'test'\n",
    "    for inputs, labels in podaci[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.float())\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "        for i in range(len(preds)):\n",
    "            if (preds[i] == labels.data[i]):\n",
    "                if (preds[i] == 1):\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            else:\n",
    "                if (preds[i] == 1):\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels.data).sum()\n",
    "\n",
    "    epoch_loss = running_loss / 201\n",
    "    epoch_acc = 100 * float(correct) / total\n",
    "    mcc = (tn * tp-fp*fn) / math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)) \n",
    "    print(f'MCC: {mcc: .4f}')\n",
    "    print(f'Test Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    putanja = \"C:/Users/Zephyrus/Desktop/cnn data/data\"\n",
    "    #niz od podataka za svaku sekvencu (podatak je niz od id-ja, sekvence i labele)\n",
    "    podaci = ucitavanje_podataka(putanja)\n",
    "\n",
    "    vektorizovani_podaci = vektorizacija(podaci)\n",
    "    X_train, y_train, X_test, y_test = podela_podataka(vektorizovani_podaci)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    X_test2 = X_test[:20]\n",
    "    y_test2 = y_test[:20]\n",
    "\n",
    "    X_test1 = X_test[20:]\n",
    "    y_test1 = y_test[20:]\n",
    "\n",
    "    dataset_sizes = {'train': len(X_train),\n",
    "                    'val': len(X_test1),\n",
    "                     'test': len(X_test2)}\n",
    "\n",
    "    dataset = {\n",
    "        'train': [X_train, y_train],\n",
    "        'val': [X_test, y_test]\n",
    "    }\n",
    "\n",
    "    data_dir = \"C:/Users/Zephyrus/Desktop/cnn data\"\n",
    "    path1 = \"C:/Users/Zephyrus/Desktop/Model/state_dict_model.pt\"\n",
    "\n",
    "    # basic error checking to check whether you correctly unzipped the dataset into the working directory\n",
    "    assert os.path.exists(data_dir), f'Could not find {data_dir} in working directory {os.getcwd()}n'\n",
    "    dirs_exist = [os.path.exists(os.path.join(data_dir, x)) for x in ['train', 'val']]\n",
    "    assert all(dirs_exist), f'Could not find train/val dirs check if you have train and val directly under {data_dir}.'\n",
    "    data_numpy_list = {}\n",
    "    data_numpy_list['train'] = [x for x in glob.glob(os.path.join(data_dir, 'train/**/*.npy'), recursive=True)]\n",
    "    data_numpy_list['val'] = [x for x in glob.glob(os.path.join(data_dir, 'val/**/*.npy'), recursive=True)]\n",
    "   \n",
    "    # ImageFolder is a PyTorch class - it expects <class1-name>, <class2-name>, ...folders under the root path you give it\n",
    "    datasets = {x: NumpyDataset(data_numpy_list[x], data_transforms) for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=BATCH_SIZE, shuffle=True) for x in ['train', 'val']}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    custom_cnn = CustomCNN().to(device)\n",
    "    optimizer_conv = optim.Adam(filter(lambda p: p.requires_grad, custom_cnn.parameters()))\n",
    "\n",
    "    print(f\"number of params in model {count_parameters(custom_cnn)}\")\n",
    "    model_conv, metrics = train_model(custom_cnn, criterion, optimizer_conv, num_epochs=100, path=path1, dataloaders=dataloaders)\n",
    "\n",
    "    train_loss = np.array(metrics['train_loss'])\n",
    "    val_loss = np.array(metrics['val_loss'])\n",
    "    train_acc = np.array(metrics['train_acc'])\n",
    "    val_acc = np.array(metrics['val_acc'])\n",
    "\n",
    "    np.savetxt('train_loss.csv', train_loss, delimiter=',')\n",
    "    np.savetxt('val_loss.csv', val_loss, delimiter=',')\n",
    "    np.savetxt('train_acc.csv', train_acc, delimiter=',')\n",
    "    np.savetxt('val_acc.csv', val_acc, delimiter=',')\n",
    "\n",
    "    podaci_za_test = [x for x in glob.glob(os.path.join(data_dir, 'test/**/*.npy'), recursive=True)]\n",
    "    datasets1 = {x: NumpyDataset(podaci_za_test, data_transforms) for x in ['test']}\n",
    "    podaci = {x: torch.utils.data.DataLoader(datasets1[x], batch_size=BATCH_SIZE, shuffle=True) for x in\n",
    "              ['test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ahishdba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15304/815843543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m             X, y = self._validate_data(X, y, accept_sparse=\"csr\",\n\u001b[1;32m-> 1132\u001b[1;33m                                        multi_output=True)\n\u001b[0m\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    804\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 646\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zephyrus\\anaconda3\\envs\\psiml1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     (type_err,\n\u001b[1;32m--> 100\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    101\u001b[0m             )\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "putanja = \"C:/Users/Zephyrus/Desktop/cnn data/data\"\n",
    "podaci = ucitavanje_podataka(putanja)\n",
    "\n",
    "vektorizovani_podaci = vektorizacija(podaci)\n",
    "X_train, y_train, X_test, y_test = podela_podataka(vektorizovani_podaci)\n",
    "X_train = np.array(X_train).reshape(300, 17050)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test).reshape(100, 17050)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2, metric='euclidean')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3ff55a2362d840d3433b3a68631efa6e477163e310b6a9fd17f9078908dbfa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
